<section id="lazybridge">
    <title>Example: the <quote>Lazy</quote> bridge</title>
	<section>
		<title>The rationale of threaded bridges</title>
    	<para>    	
	    	The 'bridge' concept was introduced to cope with the ability of 
	    	the Apache HTTP web server to adopt different multiprocessing 
	    	models by loading one of the available MPMs (Multi Processing Modules). 
			A bridge's task is to let mod_rivet fit the selected multiprocessing
			model in the first place. Still separating mod_rivet core
			functions from the MPM machinery provided also a solution for
			implementing a flexible and extensible design that enables 
			a programmer to develop alternative approaches to workload and 
			resource management. 
   	</para>
   	<para>
   		The Apache HTTP web server demands its module to
   		run with any MPM irrespective of its internal architecture and its
   		a general design constrain to make no assumptions about the MPM. 
   		Unfortunately for Tcl is a relevant fact to be running as only execution
   		thread within a process (like with the 'prefork' MPM) 
   		or within one of many execution threads. First of all Tcl is itself 
   		threaded (unless threads are disabled at compile time) and many
   		of the basic Tcl data structures (namely Tcl_Obj) cannot be safely 
   		shared among threads. This demands that Tcl interpreters run 
   		on separated threads communicating with the HTTP web server 
   		through suitable methods.
   	</para>
	</section>
	<section>
	 <title>Lazy bridge data structures</title>
    <para>
	   The lazy bridge was initially developed to outline the basic tasks
    	carried out by each function making a rivet MPM bridge. 
    	The lazy bridge attempts to be minimalistic
    	but it's nearly fully functional, only a few configuration
    	directives (SeparateVirtualInterps and SeparateChannel)
    	are ignored because fundamentally incompatible. 
    	The bridge is experimental but perfectly fit for many applications,
    	for example it's good on development machines where server restarts
    	are frequent. 
    </para>

    <para>
    	This is the lazy bridge jump table, as such it defines the functions
    	implemented by the bridge.
    </para>
    
    <programlisting>RIVET_MPM_BRIDGE {
    NULL,
    Lazy_MPM_ChildInit,
    Lazy_MPM_Request,
    Lazy_MPM_Finalize,
    Lazy_MPM_ExitHandler,
    Lazy_MPM_Interp
};</programlisting>

	<para>
		After the server initialization stage, child processes read the configuration 
		and modules build their own configuration representation. MPM bridges hooks into
		this stage to store and/or build data structures relevant to their design.
		A fundamental information built during this stage is the database of virtual hosts.
		The lazy bridge keeps an array of virtual host descriptor pointers
		each of them referencing an instance of the following structure.
	</para>
	<programlisting>/* virtual host descriptor */

typedef struct vhost_iface {
    int                 idle_threads_cnt;   /* idle threads for the virtual hosts       */
    int                 threads_count;      /* total number of running and idle threads */
    apr_thread_mutex_t* mutex;              /* mutex protecting 'array'                 */
    apr_array_header_t* array;              /* LIFO array of lazy_tcl_worker pointers   */
} vhost;</programlisting>

 	<para>
 		A pointer to this array is stored in the bridge status structure which a basic
 		structure that likely every bridge has to create.
	</para>
	<programlisting>/* Lazy bridge internal status data */

typedef struct mpm_bridge_status {
    apr_thread_mutex_t* mutex;
    int                 exit_command;
    int                 exit_command_status;
    int                 server_shutdown;    /* the child process is shutting down  */
    vhost*              vhosts;             /* array of vhost descriptors          */
} mpm_bridge_status;</programlisting>
	<para>
		By design the bridge must create exactly one instance of this structure and store the pointer
		to it in <command>module_globals->mpm</command>. This is usually done
		at the very beginning of the child init script function pointed by 
		<command>mpm_child_init</command> in
		the <command>rivet_bridge_table</command> structure. For the lazy bridge this field
		in the jump table points to <command>Lazy_MPM_ChildInit</command>
	</para>
	<programlisting>void Lazy_MPM_ChildInit (apr_pool_t* pool, server_rec* server)
{
    ...
 	  
    module_globals->mpm = apr_pcalloc(pool,sizeof(mpm_bridge_status));
    
    ....
}</programlisting>
	</section>
	<section>
	 <title>Handling Tcl's exit core command</title> 
	<para>
		Most of the fields in the <command>mpm_bridge_status</command> are meant to deal 
		with the child exit process. Rivet supersedes the Tcl core's exit function
		with a <command>::rivet::exit</command> function and it does so in order to curb the effects
		of the core function that would force a child process to immediately exit. 
		This could have unwanted side effects, like skipping the execution of important
		code dedicated to release locks or remove files. For threaded MPMs the abrupt
		child process termination could be even more disruptive as all the threads
		will be deleted without warning.	
	</para>
	<para>
		The <command>::rivet::exit</command> implementation calls the function pointed by
		<command>mpm_exit_handler</command> which is bridge specific. Its main duty
		is to take the proper action in order to release resources and force the
		bridge controlled threads to exit.  
	</para>
	<note>
		Nonetheless the <command>exit</command> command should be avoided in ordinary mod_rivet
		programming. We cannot stress this point enough. If your application must bail out
		for some reason focus your attention on the design to find the most appropriate
		route to exit and whenever possible avoid 
		calling <command>exit</command> at all (which basically wraps a
		C call to Tcl_Exit). Anyway the Rivet implementation partially transforms
		<command>exit</command> in a sort of special <command>::rivet::abort_page</command>
		implementation whose eventual action is to call the <command>Tcl_Exit</command>
		library call. See the <command><xref linkend="exit">::rivet::exit</xref></command>
		command for further explanations.
	</note>
	<para>
		Both the worker bridge and lazy bridge 
		implementations of <command>mpm_exit_handler</command> call the function pointed 
		by <command>mpm_finalize</command> which also the function called by the framework 
		when the web server shuts down.
		See these functions' code for further details, they are very easy to 
		read and understand
	</para>
	</section>
	<section>
		<title>HTTP request processing with the lazy bridge</title>
	<para>
		Requests processing with the lazy bridge is done by determining for which
		virtual host a request was created. The <command>rivet_server_conf</command>
		structure keeps a numerical index for each virtual host. This index is used
		to reference the virtual host descriptor and from it the request
		handler tries to gain lock on the mutex protecting the array of <command>lazy_tcl_worker</command>
		structure pointers. Each instance of this structure is a descriptor of a thread created for
		a specific virtual host; threads available for processing have their descriptor
		on that array and the handler callback will pop the first
		<command>lazy_tcl_worker</command> pointer to signal the thread
		there is work to do for it. This is the <command>lazy_tcl_worker</command> structure
	</para>
	<programlisting>/* lazy bridge Tcl thread status and communication variables */

typedef struct lazy_tcl_worker {
    apr_thread_mutex_t* mutex;
    apr_thread_cond_t*  condition;
    int                 status;
    apr_thread_t*       thread_id;
    server_rec*         server;
    request_rec*        r;
    int                 ctype;
    int                 ap_sts;
    int                 nreqs;
    rivet_server_conf*  conf;               /* rivet_server_conf* record            */
} lazy_tcl_worker;</programlisting>
	<para>
		The server field is assigned with the virtual host server record. Whereas the <command>conf</command>
		field keeps the pointer to a run time computed <command>rivet_server_conf</command>. This structure
		may change from request to request because <command>&lt;Directory ...&gt;...&lt;/Directory&gt;</command> could
		change the configuration with directory specific directive values
	</para>
	<para>
		The Lazy bridge will not start any Tcl worker thread at server startup, but it will
		wait for requests to come in and they are handed down to a worker threads by popping 
		a lazy_tcl_worker pointer from the related array in the virtual hosts database or,
		in case the array is empty and no threads are available, a new worker thread is 
		created. The code in the <command>Lazy_MPM_Request</command> function
	</para>
	<programlisting>    lazy_tcl_worker*    w;
	 ...
    apr_array_header_t* array;
    apr_thread_mutex_t* mutex;

    mutex = module_globals->mpm->vhosts[conf->idx].mutex;
    array = module_globals->mpm->vhosts[conf->idx].array;
    apr_thread_mutex_lock(mutex);
    
    ...
    
    /* If the array is empty we create a new worker thread */

    if (apr_is_empty_array(array))
    {
        w = create_worker(module_globals->pool,r->server);
        (module_globals->mpm->vhosts[conf->idx].threads_count)++; 
    }
    else
    {
        w = *(lazy_tcl_worker**) apr_array_pop(array);
    }
    apr_thread_mutex_unlock(mutex);

    ...</programlisting>
	<para>
		After a request is processed the Tcl worker thread returns its own
		lazy_tcl_worker descriptor to the array and then waits
		on the condition variable used to control and synchronize the bridge 
		threads with the Apache worker threads.
	</para>
	<programlisting>     /* rescheduling itself in the array of idle threads */
       
     apr_thread_mutex_lock(module_globals->mpm->vhosts[idx].mutex);
     *(lazy_tcl_worker **) apr_array_push(module_globals->mpm->vhosts[idx].array) = w;
     apr_thread_mutex_unlock(module_globals->mpm->vhosts[idx].mutex);</programlisting>	
	<para>
		The lazy bridge <command>module_globals->bridge_jump_table->mpm_thread_interp</command>, which
		is supposed to return the rivet_thread_interp structure pointer relevant to a given
		request, has a straightforward task to do since by design each thread has
		one interpreter
	</para>
	<programlisting>rivet_thread_interp* Lazy_MPM_Interp(rivet_thread_private *private,
                                     rivet_server_conf* conf)
{
    return private->ext->interp;
}</programlisting>
	<para>
		As already pointed out
		running this bridge you get separate virtual interpreters and separate channels by default
		and since by design each threads gets its own Tcl interpreter and Rivet channel you will
		not be able to revert this behavior in the configuration with 
	</para>
	<programlisting>SeparateVirtualInterps Off
SeparateChannels       Off</programlisting>
	<para>
		which are simply ignored
	</para>
	</section>
 </section>