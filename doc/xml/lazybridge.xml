<section id="lazybridge">
    <title>Example: the <quote>Lazy</quote> bridge</title>
    <para>
    	The lazy bridge was developed to show how a threaded bridge can
    	implement a simple yet almost fully functional threading model to
    	handle requests.
    </para>
    <para>
    	The lazy bridge implements all the functions (except for one)
    	defined in <command>rivet_bridge_table</command>. Therefore is
    	a good starting point to understand what a Rivet MPM bridge is
    	supposed to do and how to implement other threading models in
    	order to serve requests. The Lazy bridge implement the following
    	functions
    </para> 
    <programlisting>RIVET_MPM_BRIDGE {
    NULL,
    Lazy_MPM_ChildInit,
    Lazy_MPM_Request,
    Lazy_MPM_Finalize,
    Lazy_MPM_ExitHandler,
    Lazy_MPM_Interp
};</programlisting>
	<para>
		The bridge status has to be stored in the <command>mpm_bridge_status</command>.
	</para>
	<programlisting>/* Lazy bridge internal status data */

typedef struct mpm_bridge_status {
    apr_thread_mutex_t* mutex;
    int                 exit_command;
    int                 exit_command_status;
    int                 server_shutdown;    /* the child process is shutting down  */
    vhost*              vhosts;             /* array of vhost descriptors          */
} mpm_bridge_status;</programlisting>
	<para>
		By design each bridge must create exactly one instance of this structure and store the pointer
		to the memory holding it in <command>module_globals->mpm</command>. This is usually done
		at the very beginning of the function pointed by <command>mpm_child_init</command> in
		the <command>rivet_bridge_table</command> structure. For the lazy bridge the this field
		in the jump table points to <command>Lazy_MPM_ChildInit</command>
	</para>
	<programlisting>void Lazy_MPM_ChildInit (apr_pool_t* pool, server_rec* server)
{
    ...
 	  
    module_globals->mpm = apr_pcalloc(pool,sizeof(mpm_bridge_status));
    ....
}</programlisting>
	<para>
		Most of the fields in the <command>mpm_bridge_status</command> are meant to deal 
		with the child exit process, either beacuse a <command>::rivet::exit</command> was called
		or because the Apache web server framework required the child process to exit. This 
		is handled by the function pointed by <command>mpm_finalize</command> (Lazy_MPM_Finalize for
		the lazy bridge). The <command>::rivet::exit</command> command forces the bridge to 
		initiate the exit sequence by calling the <command>mpm_exit_handler</command> 
		(See function <command>Lazy_MPM_ExitHandler</command> in the lazy bridge code for
		further details)
	</para>
	<para>
		After the server initialization stage the Apache HTTP Web Server starts 
		child processes and each of them in turn will read the configuration. 
		A fundamental information built during this stage is the database of virtual hosts.
		The lazy bridge keeps an array of virtual host descriptor pointers 
		<command>vhosts*</command> each of them referencing an instance of the following structure.
	</para>
	<programlisting>/* virtual host descriptor */

typedef struct vhost_iface {
    int                 idle_threads_cnt;   /* idle threads for the virtual hosts       */
    int                 threads_count;      /* total number of running and idle threads */
    apr_thread_mutex_t* mutex;              /* mutex protecting 'array'                 */
    apr_array_header_t* array;              /* LIFO array of lazy_tcl_worker pointers   */
} vhost;</programlisting>
	<para>
		Each virtual host descriptor will maintain 
		a list of threads referenced through their <command>lazy_tcl_worker</command>
		structure pointers stored in a APR array container. The handler callback will determine
		which virtual host a request belongs to and then it will gain lock on the APR array
		to pop the first <command>lazy_tcl_worker*</command> pointer to signal the thread
		there is work to do for it. This structure keeps the basic data needed for the 
		inter-thread communication. 
	</para>
	<programlisting>/* lazy bridge Tcl thread status and communication variables */

typedef struct lazy_tcl_worker {
    apr_thread_mutex_t* mutex;
    apr_thread_cond_t*  condition;
    int                 status;
    apr_thread_t*       thread_id;
    server_rec*         server;
    request_rec*        r;
    int                 ctype;
    int                 ap_sts;
    int                 nreqs;
    rivet_server_conf*  conf;               /* rivet_server_conf* record            */
} lazy_tcl_worker;</programlisting>
	<para>
		The Lazy bridge will not start any Tcl worker thread at server startup, but it will
		wait for requests to come in and they are handed down to a worker threads by popping 
		a lazy_tcl_worker pointer from the related array in the virtual hosts database or,
		in case the array is empty and no threads are available, a new worker thread is 
		created. The code in the <command>Lazy_MPM_Request</command> function
	</para>
	<programlisting>    lazy_tcl_worker*    w;
	 ...
    apr_array_header_t* array;
    apr_thread_mutex_t* mutex;

    mutex = module_globals->mpm->vhosts[conf->idx].mutex;
    array = module_globals->mpm->vhosts[conf->idx].array;
    apr_thread_mutex_lock(mutex);
    
    ...
    
    /* If the array is empty we create a new worker thread */

    if (apr_is_empty_array(array))
    {
        w = create_worker(module_globals->pool,r->server);
        (module_globals->mpm->vhosts[conf->idx].threads_count)++; 
    }
    else
    {
        w = *(lazy_tcl_worker**) apr_array_pop(array);
    }
    apr_thread_mutex_unlock(mutex);

    ...</programlisting>
	<para>
		After a request is processed the Tcl worker thread returns its own
		lazy_tcl_worker descriptor to the array and then starts to wait
		on the condition variable used to control and synchronize the interplay
		of the 2 threads.
	</para>
	<programlisting>
     /* rescheduling itself in the array of idle threads */
       
     apr_thread_mutex_lock(module_globals->mpm->vhosts[idx].mutex);
     *(lazy_tcl_worker **) apr_array_push(module_globals->mpm->vhosts[idx].array) = w;
     apr_thread_mutex_unlock(module_globals->mpm->vhosts[idx].mutex);</programlisting>	
	<para>
		The lazy bridge <command>module_globals->bridge_jump_table->mpm_thread_interp</command>, which
		is supposed to return the rivet_thread_interp structure pointer relevant to a given
		request, has a straightforward task to do since by design each thread has
		one interpreter
	</para>
	<programlisting>rivet_thread_interp* Lazy_MPM_Interp(rivet_thread_private *private,
                                     rivet_server_conf* conf)
{
    return private->ext->interp;
}</programlisting>
	<para>
		Running this bridge you get separate virtual interpreters and separate channels by default
		and since by design each threads gets its own Tcl interpreter and Rivet channel you will
		not be able to revert this behavior in the configuration with 
	</para>
	<programlisting>SeparateVirtualInterps Off
SeparateChannels       Off</programlisting>
	<para>
		which are simply ignored
	</para>
 </section>